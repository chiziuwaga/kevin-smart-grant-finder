# End-to-End Testing Plan

This document outlines the manual testing procedure to ensure the Kevin Smart Grant Finder application is fully functional after the major refactoring and deployment.

## Objective

To verify that the entire application workflow, from user interaction on the frontend to data processing and retrieval on the backend, is working as expected. This includes testing the new Perplexity and OpenAI integration, the recursive research agent, and the cleaned-up codebase.

## Pre-requisites

1.  **Backend Server Running:** The FastAPI backend server is running and accessible.
2.  **Frontend Application Running:** The React frontend is running and accessible in a browser.
3.  **Environment Variables:** All necessary environment variables (API keys, database URLs, etc.) are correctly set for both frontend and backend.

## Testing Steps

### 1. Frontend Smoke Test

- **Action:** Open the application in a web browser.
- **Expected Result:** The main page loads without any visible errors. The layout is correct, and all static elements (headers, footers, buttons) are displayed properly.

### 2. User Profile and Configuration

- **Action:** Navigate to the user profile or settings page.
- **Expected Result:** The user profile information is displayed correctly. If there are configurable settings (e.g., research preferences), they should be visible and editable.

### 3. Grant Search Functionality

- **Action:** Enter a search query for grants (e.g., "grants for AI research in healthcare").
- **Expected Result:**
  - The search is initiated, and a loading indicator is displayed.
  - The backend API for grant search is called (verify in browser's developer tools network tab).
  - The recursive research agent is triggered on the backend.
  - The Perplexity API is called for research.
  - The OpenAI API is called for data extraction.
  - A list of grants is returned and displayed on the frontend.
  - The grant information is well-structured and contains the expected fields (title, description, amount, deadline, etc.).

### 4. Grant Details and Analysis

- **Action:** Click on a grant from the search results to view its details.
- **Expected Result:**
  - A detailed view of the grant is displayed.
  - The analysis generated by the agents (e.g., compliance analysis, summary) is shown.
  - All data is formatted correctly.

### 5. Deduplication Verification

- **Action:** Perform multiple similar searches.
- **Expected Result:** The system should identify and handle duplicate grants, ensuring that the same grant is not repeatedly shown as a new result. Check for consistency in grant IDs or other unique identifiers.

### 6. Error Handling

- **Action:**
  - Try a search query that is unlikely to return results.
  - If possible, simulate an API failure (e.g., by temporarily disabling an API key).
- **Expected Result:**
  - For a query with no results, a user-friendly message is displayed.
  - In case of an API failure, a graceful error message is shown to the user, and the application does not crash.

### 7. Backend Log Verification

- **Action:** While performing the above tests, monitor the backend server logs.
- **Expected Result:**
  - Logs should indicate the start and end of each major process (e.g., grant search, analysis).
  - Calls to external services (Perplexity, OpenAI) should be logged.
  - Any errors or warnings should be clearly logged, providing enough context for debugging.

### 8. Heroku Deployment Health Check

- **Action:** Access the deployed Heroku application's health check endpoint (`/health`).
- **Expected Result:** The endpoint returns a success status, indicating that the deployed application is healthy.

## Post-Testing

- Document any bugs or issues found with detailed steps to reproduce them.
- If all tests pass, the application is considered ready for production use.
